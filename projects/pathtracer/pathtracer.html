---
title: Pathtracer
layout: page
hide: true
---

<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/intro/bunny.png" align="middle" width="500"/>
        <figcaption align="middle">CBbunny Adaptive Sampled (2048 Samples per Pixel)</figcaption>
      </td>
      <td>
        <img src="images/intro/CBspheres_lambertian.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres Adaptive Sampled (2048 Samples per Pixel)</figcaption>
      </td>
    </tr>
  </table>
</div>

<br><br>

<div>

<h2 align="middle">Overview</h2>
<p class = "medium">
PathTracer was an extremely interesting introduction to the wonderful world of light in computer graphics. Incredibly intuitive, but equally powerful, Pathtracing is a
technique to simulate virtual light, and the way it interacts with the world. I learned about the general ray tracing pipeline, material simulation and optics (reflection,
and BSDF), ray and intersection geometry (Moller-Trumbore algorithm), and PathTracing optimizations (BVH and Adaptive Sampling).

</p>

<br><br>

<h2 align="middle">Section I: Intersection Testing, Ray Generation, Acceleration Structures</h2>

<h3 align="middle">Part 1: Ray Generation and Scene Intersection:</h3>


<div align = "left">
<p class = "medium">
<b><i>Overview of the Ray Casting Process:</i></b>
</div>
</p>
<div align = "justify">
<p class = "medium">
We can imagine looking at a grid of pixels. For each pixel <var>(x, y)</var> we need to somehow take an estimate of the radiance for this pixel, then write the corresponding
Spectrum to a frame buffer. For this process, we will use Monte Carlo estimation, an interesting, compact and powerful estimation algorithm that will enable us to take an
unbiased estimate of the radiance for each pixel. To that end, for each pixel, we will take <code>ns_aa samples</code>, where for each sample, we will take a random position in
that pixel, and shoot a ray from the camera into the world passing through the random location, and calculate the radiance that returns to us. For each of these <code>ns_aa samples</code>,
we will sum up the global radiance, by calling <code>est_radiance_global_illumination</code> then divide by the number of samples, effectively averaging the radiance for that
pixel. This is the value that will ultimately be written to the scene buffer.
</p>
</div>

<br>

<div align = "left">
<p class = "medium">
<b><i>Camera Ray Generation:</i></b>
</div>
</p>
<div align = "justify">
<p class = "medium">
Of course, we can’t appeal to this abstract notion of camera ray generation, so let us concretize what that truly means. To generate the camera rays themselves, once we get the
normed world space pixel coordinates (normed by screen width and height dimensions), we first need to determine the map into camera space. This is relatively simple, since as
long as we know the horizontal and vertical fields of view <code>hFov</code> and <code>vFov</code> respectively), we can simply correspond the top right of world space (1, 1)
to <code>(tan(0.5 * hFov), tan(0.5 * vFov), -1)</code>, and the bottom left to <code>(-tan(0.5 * hFov), -tan(0.5 * vFov), -1)</code>. From here, it is easy to derive the
transformation from normed world space to camera space, and having determined the camera space coordinate of the position we passed in, we can create a vector (the direction)
from the camera to this point, then transform the origin (camera location) and direction to world space. With an origin and direction in world space, our ray is fully defined!
</p>
</div>

<br>

<div align = "left">
<p class = "medium">
<b><i>Primitive Intersection:</i></b>
</div>
</p>
<div align = "justify">
<p class = "medium">
Our pathtracing algorithm depends on the ability to deal with intersections with primitives in our scene, and as such, we implement procedures to handle intersections with
triangles, and intersections with spheres. At a high level, when a ray is cast into the scene, we will test the primitives in its path, searching for intersections. Every
primitive’s intersect method requires a ray, and an intersection object. If we determine that the ray in fact intersects the primitive, and has not yet intersected another
primitive (the time field of the intersection object is not less than the intersection time with the primitive), we will take the lowest valid time of intersection, and copy
this value to the intersection object’s time field.  We will further provide references to the primitive of intersection, the bsdf of the primitive intersected, and the normal
to the primitive. In this way, intersection objects track all the necessary information to eventually handle light-object reflections.
</p>
</div>

<br>

<div align = "left">
<p class = "medium">
<b><i>Intersecting Triangles:</i></b>
</div>
</p>

<div align = "justify">
<p class = "medium">
Since we’ll end up intersecting millions of triangles during our stint with pathtracing, we’ll seek to optimize the ray-triangle intersection as much as possible. To that end,
we will leverage and implement the Moller-Trumbore algorithm. The Moller-Trumbore algorithm hinges on basic linear algebra, and is far more complicated than I give it credit;
a much more thorough explanation can be found <a href="https://www.scratchapixel.com/lessons/3d-basic-rendering/ray-tracing-rendering-a-triangle/moller-trumbore-ray-triangle-intersection" target = "_blank">here</a>.
</p>
</div>

<div align = "justify">
<p class = "medium">
Given a triangle defined by points <code>p0</code>, <code>p1</code> and <code>p2</code>, and a ray defined by origin <code>O</code> and direction <code>D</code>, parametrized by
time <code>t</code>, we can leverage barycentric coordinates to determine that an intersection of the two satisfies the equality: <code>O + tD = p0 + b1(p1 - p0) + b2(p2 - p0)</code>.
Recasting this equation as a matrix, we can thus determine the equality: <code>Mx = b</code> where <code>M = [-D, p1 - p0, p2 - p0]</code>, <code>x = [t, b1, b2]</code>,
and <code>b = O - p0</code>. A simple application of Cramer’s Rule allows us to determine that with a triangle-ray intersection, <code>x = 1 / [S1 * E1] x [dot(S2, E2), dot(S1, S) , dot(S2, D)]</code>;
<code>E1 = p1 - p0, E2 = p2 - p0, S = O - p0, S1 = D x E2, S2 = S x E1</code>. This will allow us to determine <code>t</code>, <code>b1</code>, and <code>b2</code>,
and since <code>t</code> is a recast of time, and <code>b1</code> and <code>b2</code> are based on the formulation of barycentric coordinates, this implies we see a valid
triangle ray intersection only if <code>t</code> is equal to or greater than 0, <code>b1</code> and <code>b2</code> are between 0 and 1 inclusive, and <code>(1 - b1 - b2)</code>
is less than or equal to 1. Given any ray and triangle, we thus can easily determine whether the ray intersects the triangle using the Moller-Trumbore algorithm as defined.
</p>
</div>

<br><br>

<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task1/CBgems.png" align="middle" width="500"/>
        <figcaption align="middle">CBgems.dae</figcaption>
      </td>
      <td>
        <img src="images/task1/CBspheres.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae</figcaption>
      </td>
    </tr>
  </table>
</div>

<br><br>


<h3 align="middle">Part 2: The Bounding Volume Hierarchy (BVH):</h3>

<div align = "left">
<p class = "medium">
<b><i>An Overview of BVH Generation:</i></b>
</div>
</p>

<div align = "justify">
<p class = "medium">
Intuitively, the BVH construction algorithm aims to cut the elements into partitions in a way that can accelerate our ray-primitive intersection tests. Our construction function
takes as input a <code>start</code> iterator, an <code>end</code> iterator, and a maximal leaf size. The start and end iterators essentially allow us to iterate over all the
primitives that are either in this node (the root), or the subnodes of this node. We can abstract away the fact that the complete 1D vector of primitives is stored only once
in memory, and all iterators are in fact referencing this underlying data structure. The first step in the algorithm is to determine the number of elements. We will iterate over
all the primitives using our start and end iterators and simultaneously create a bounding box with all the primitives encapsulated, and count the number of primitives.
If the number of primitives is fewer than the maximal leaf size, we can just make a leaf with the complete bounding box, with no left or right subnode, whose start and end
iterators point to the start and end iterators we passed to the function.
</p>
</div>

<div align = "justify">
<p class = "medium">
Otherwise, we know we have more elements that can be efficiently captured in one leaf, so we must generate a larger tree structure. To this end, we will first determine the
longest axis of our primitives bounding box, since we will partition the elements depending on their position along the longest axis. Without loss of generality, assume the
x-axis is the longest axis. We calculate the midpoint of this axis, then split the elements into two vectors, left and right, where elements in the left vector are primitives
whose x-coordinate is less than or equal to the midpoint we calculated, and primitives in the right vector have a larger x-coordinate than the midpoint. There is a small issue.
To avoid an infinite recursion overflow, we have to avoid cases that leave either the left or right subvectors empty. A simplistic (and admittedly poorly optimized) way of doing
this is randomly cutting our collection of primitives such that half of the primitives are set to the left vector, and the other half to the right vector.
</p>
</div>

<div align = "justify">
<p class = "medium">
We must now reformat the primitives in the complete primitives vector (remember, this vector is stored only once in memory) such that the iterators mirror the state of the
BVH. What this means is that we simply need to determine the index in the complete primitives vector of the start iterator and the end iterator. Then we will copy the elements
in the left vector to the complete primitives vector from the <code>start</code> index, to the <code>start + left.size()</code> index, and copy the elements in the right vector
from the <code>start + left.size()</code> index to the <code>end</code> iterator’s index.
</p>
</div>

<div align = "justify">
<p class = "medium">
Having reformatted the underlying complete primitives vector, we can now recursively instantiate the left and right branch of our BVH tree. We will set the <code>left</code> field of our root
node to the result of constructing a BVH with start iterator equal to the <code>start</code> iterator passed in to the root construct, and <code>end</code> iterator equal to the
<code>start iterator + left.size()</code>. We will set the right field of our root node to the result of constructing a BVH with <code>start</code> iterator equal to
<code>start + left.size()</code>, and an <code>end</code> iterator equal to the <code>end</code> iterator passed into the root construct. This allows us to recursively construct our BVH!
</p>
</div>

<br><br>

<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task2/cow.png" align="middle" width="500"/>
        <figcaption align="middle">cow.dae</figcaption>
      </td>
      <td>
        <img src="images/task2/maxplanck.png" align="middle" width="500"/>
        <figcaption align="middle">maxplanck.dae</figcaption>
      </td>
    </tr>
  </table>
</div>

<br><br>

<br><br>

<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task2/CBdragon.png" align="middle" width="500"/>
        <figcaption align="middle">CBdragon.dae</figcaption>
      </td>
      <td>
        <img src="images/task2/CBbunny.png" align="middle" width="500"/>
        <figcaption align="middle">CBbunny.dae</figcaption>
      </td>
    </tr>
  </table>
</div>

<br><br>

<br><br>

<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task2/CBlucy.png" align="middle" width="500"/>
        <figcaption align="middle">CBlucy.dae</figcaption>
      </td>
      <td>
        <img src="images/task2/peter.png" align="middle" width="500"/>
        <figcaption align="middle">peter.dae</figcaption>
      </td>
    </tr>
  </table>
</div>

<br><br>

<div align = "middle">
  <table border = "1">
    <tr>
      <th></th>
	  <th>Render Time - Unoptimized (seconds)</th>
      <th>Render Time - BVH Optimized (seconds)</th>
    </tr>
	<tr>
      <th>cow.dae</th>
	  <th>29.0670</th>
      <th>0.2432</th>
    </tr>
    <tr>
      <th>maxplanck.dae</th>
	  <th>292.7574</th>
      <th>0.3799</th>
    </tr>
	<tr>
      <th>CBbunny.dae</th>
	  <th>199.8900</th>
      <th>0.2250</th>
    </tr>
    <tr>
      <th>CBdragon.dae</th>
	  <th>406.0455</th>
      <th>0.2220</th>
    </tr>
	<tr>
      <th>peter.dae</th>
	  <th>N/A</th>
      <th>0.2851</th>
    </tr>
    <tr>
      <th>CBlucy.dae</th>
	  <th>N/A</th>
      <th>0.2271</th>
    </tr>
  </table>
</div>

<br><br>

<div align = "left">
<p class = "medium">
<b><i>Timing Comparison:</i></b>
</div>
</p>

<div align = "justify">
<p class = "medium">
As you can clearly see from the table above, the difference is stark, and immediately it is clear why we apply this sort of optimization to intersection tests. Take for instance
CBdragon.dae, which renders in about 0.2271 seconds with our BVH optimization. And without? It renders at an <i>abysmal</i> 406.0455 seconds, which is almost 7 minutes. This means BVH has
sped up rendering times by an incredible factor of <b>1787</b>.  Rendering scenes with normal shading, even without tracing light can already take minutes without our BVH optimization, but
less than a second with. Intuitively it should be relatively clear why creating this hierarchy is so effective. If we imagine a single ray traversing our scene, it is highly unlikely it
will hit every triangle in the scene. Spatial locality is a huge deal in ray tracing, so we shouldn’t waste our time checking triangles that will evidently never be hit. For each collection
of primitives, we track the minimal bounding box that encapsulates these primitives. If we can quickly determine that the ray doesn’t intersect the bounding box, we don’t need to intersect
test any of the triangles encapsulated by the bounding box, and we won’t need to test any of the primitives in any of the subnodes rooted at this node either. Essentially, we can avoid
testing millions of primitives just with this simple optimization. Of course, this isn’t a perfect solution. Since bounding volumes are split by primitive collection, not pure space,
there are some inefficiencies with overlapping bounding volumes, for which we need to intersect test both overlapping volumes, however, in our unoptimized version, we must test all
primitives with each ray, which is clearly far worse.

</p>
</div>

<br><br><br><br>

<h2 align="middle">Section II: Lighting Calculations or The Heart of PathTracing</h2>

<h3 align="middle">Part 3: Direct Illumination:</h3>

<br><br>
<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task3/CBspheres_UHL.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae UHL</figcaption>
      </td>
      <td>
        <img src="images/task3/CBspheres_LIS.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae LIS</figcaption>
      </td>
    </tr>
  </table>
</div>

<br><br>

<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task3/CBbunny_UHL.png" align="middle" width="500"/>
        <figcaption align="middle">CBbunny.dae UHL</figcaption>
      </td>
      <td>
        <img src="images/task3/CBbunny_LIS.png" align="middle" width="500"/>
        <figcaption align="middle">CBbunny.dae LIS</figcaption>
      </td>
    </tr>
  </table>
</div>
<br><br>



<div align = "left">
<p class = "medium">
<b><i>Uniform Hemisphere Lighting:</i></b>
</div>
</p>

<div align = "justify">
<p class = "medium">
To implement Uniform Hemisphere Lighting (UHL), we assume, from part 1, we have already traced a ray from our camera into our scene, and we’ve found an intersection! The question
now is: is there radiance that travels from a light source to our intersection, such that it will bounce back along the ray we’ve traced to the camera, lighting up the pixel? We
will again employ Monte Carlo Estimation here to attempt to approximate the intersection irradiance, which will thus tell us how much light will bounce off our intersected object,
to our camera.
</p>
</div>

<div align = "justify">
<p class = "medium">
We implement <code>estimate_direct_lighting_hemisphere</code>, which is passed a ray object <code>r</code>, and an intersection object that represents the intersection of the
ray with some primitive. Tracing the Monte Carlo estimation procedure, for each of our <code>n</code> samples, we will calculate a random direction in the hemisphere of our
intersection, then throw a ray <code>s</code> in the direction of our random sample, from the hit point (intersection location). Then we check to see if <code>s</code> hits a
light source. If we intersect an object <code>q</code> that isn’t a light source, that means <code>q</code> casts a shadow on our initial intersection point, but if the light
emission of the newly intersected object is positive, this means we’ve hit a light! We can get the emission of the newly intersected object, and since radiance is conserved
through the ray, we can calculate the radiance that will be passed to the camera along <code>r</code> by employing the reflection equation; namely, the light that will be
transported to the camera supposing irradiance <code>E</code> will be <code>2PI * bsdf(dir_camera, dir_light) * E * cos(angle between light and intersection primitive)</code>.
Calculating this for each of our samples, we simply average the radiance that reaches the camera to generate our UHL for each pixel.
</p>
</div>

<br>

<div align = "left">
<p class = "medium">
<b><i>Light Importance Sampling:</i></b>
</div>
</p>

<div align = "justify">
<p class = "medium">
To implement Light Importance Sampling (LIS) we will perform roughly the same process, the difference being, we will eschew sampling randomly in a hemisphere in favor of sampling
only in the area of the light elements. This is like the difference of first aiming our shot before attempting to shoot a basketball vs. randomly bouncing the basketball in the
room, and hoping it lands in the basket. As you might imagine, the first option results in a drastically higher chance of actually ending up in the basket. As with UHL, we first
start with a ray we’ve traced into the scene, that has intersected some primitive <code>p</code>. From <code>p</code> we will randomly sample each of the lights. After all, for
direct lighting, the only light that will ever reach <code>p</code> will be from one of the light sources, so why bother sampling the floor or the walls (this will change when
we introduce indirect lighting)? For each of the lights in the scene we will sample the light <code>ns_area_light times</code>, by first sampling a random direction that points
to the area of the light. We will then check again to make sure that we aren’t hitting some other primitive first, which would thus mean we’d be shadowed at this point on
<code>p</code>, and if not, we can now calculate the outgoing radiance just as with UHL (remember the reflection equation?). We do have to check to make sure that <code>p</code>
is facing the light, but this is as simple as checking to ensure the cosine of the angle from UHL is between 0 and Pi. If you hold out your hand with the palm facing down, your
lights won’t directly reach the palm of your hand!
</p>
</div>

<div align = "justify">
<p class = "medium">
There are two caveats to this part. In our reflection radiance calculation from UHL, we multiplied by <code>2PI</code>, but this doesn’t quite work anymore. In actuality, we
were abstractly dividing by the pdf, the probability density function of sampling such a specific direction, which in the case of a uniform hemisphere is exactly uniform
<code>(1 / 2pi)</code>, but with LIS, this is no longer the case. This isn’t really a big deal, we simply need to divide by the proper pdf everytime now, instead of using a
uniform constant pdf. The second hitch is the case of point lights. A point light has an infinitely small area of light emission, so it's clear why UHL is a bad idea. But this
also means we don’t really need to sample <code>ns_area_light samples</code>, 1 ought to be enough! Either we’re shadowed against the point light or we aren’t. Thus we can early
out of the sampling procedure, but otherwise, this case is a mirror for area light sampling.
</p>
</div>

<div align = "justify">
<p class = "medium">
Finally, after estimating the radiance arriving from the ray casted to the scene intersecting primitive <code>p</code> for each of the lights, we simply divide this total
radiance by the number of samples, to complete our Monte Carlo Estimator.
</p>
</div>

<br>

<div align = "left">
<p class = "medium">
<b><i>UHL vs. LIS Quality Comparison:</i></b>
</div>
</p>

<div align = "justify">
<p class = "medium">
While UHL and LIS are both technically valid methods of computing Direct Lighting, taking a quick look at a side by side comparison makes it pretty obvious that LIS ought to be
the preferred method. Even not considering the fact that UHL struggles to render point lit scenes, we can clearly see a plethora of noise scattered throughout the UHL renders,
and this is reasonable. To reiterate an earlier point, “after all, for direct lighting, the only light that will ever reach <code>p</code> will be from one of the light sources,
 so why bother sampling the floor or the walls?” This is at the core of the difference between the two. You can think of samples as data quality; the more <b>useful samples</b>
 you have, the higher your data quality, and thus image quality will be. Naturally, you have limited computation cycles, so you can only collect so much data.  In UHL, you throw
 away a lot of your data by sampling random non emissive points, that really don’t give you any actionable data about direct lighting, and as a result, you gain less useful
 data, lowering your image quality. If we’re always looking to collect data from useful points, light sources with positive light emission, we’ll inherently amass more data
 containing useful lighting calculations. Looking at the images at the beginning of this section, we get a clear visualization for the effects of sampling randomly, vs. sampling
 purposefully. The UHL bunny is full of graininess, wrought from light beams that were projected into non emissive surfaces. Since our samples in the LIS bunny only look where
 the light emission exists, we clear out a great deal of the grainy samples just by shifting the light beam from a non emissive surface to a light.
</p>
</div>



<br>

<div align = "left">
<p class = "medium">
<b><i>Light Ray Comparison (1 Sample Per Pixel):</i></b>
</div>
</p>

<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task3/CBspheres_l1.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae 1 Light Ray</figcaption>
      </td>
      <td>
        <img src="images/task3/CBspheres_l4.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae 4 Light Rays</figcaption>
      </td>
    </tr>
  </table>
</div>
<br><br>
<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task3/CBspheres_l16.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae 16 Light Rays</figcaption>
      </td>
      <td>
        <img src="images/task3/CBspheres_l64.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae 64 Light Rays</figcaption>
      </td>
    </tr>
  </table>
</div>

<div align = "justify">
<p class = "medium">
<i>Notice the amelioration of soft shadow rendering with more light samples, serving to blur the shadowed region, precluding granularity.</i>
</p>
</div>
<br><br>

<h3 align="middle">Part 4: Global Illumination:</h3>

<br><br>
<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task4/CBbunny_HQ.png" align="middle" width="500"/>
        <figcaption align="middle">CBbunny.dae Global Illumination (1024 Samples per Pixel)</figcaption>
      </td>
      <td>
        <img src="images/task4/Spheres_HQ.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae Global Illumination (1024 Samples per Pixel)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br><br>

<div align = "justify">
<p class = "medium">
There is noticeably still a feeling and semblance of artificiality with direct lighting only, and this is understandable; light doesn’t only travel from the source to the object
to our eyes, it will bounce from place to place, focus and diffuse. This is what we attempt to mimic, of course on a smaller scale.
</p>
</div>

<div align = "justify">
<p class = "medium">
The function <code>at_least_one_bounce_radiance</code> attempts to approximate the radiance of rays that will <b>back bounce</b>. The function is passed a ray object <code>r</code>,
and an intersection object that represents the intersection of the ray with some primitive, and will recursively attempt to calculate the total direct and indirect lighting that
will return back along <code>r</code>.
</p>
</div>

<div align = "justify">
<p class = "medium">
As before, we will consider the ray <code>r</code> that we have traced into our scene, which has intersected with primitive <code>p</code>. There are a few cases we ought to
consider. It is possible <code>p</code> itself is emissive, in which case, <code>p</code> itself will presumably emit light back through <code>r</code> into the camera. We will
call this, 0th degree lighting. Then it is possible that <code>p</code> is directly lit, namely, it is in the direct path of a light, with no shadowing elements. This is the 1st
degree lighting. Finally, it is possible that light reaches <code>p</code> that was reflected from some other object <code>q</code>, which itself has its own 1st degree lighting,
as well as light that was reflected from some <b>other</b> object <code>u</code>. This indirect lighting we will thus call 2nd degree lighting. Put more colloquially, the lighting at
point <code>p</code> is predicated on the light <code>p</code> emits, the light directly illuminating <code>p</code>, and the light that has bounced to <code>p</code> from
another point <code>q</code>, and the quantity of the radiance from <code>q</code> is itself dependent on direct, and indirect lighting at <code>q</code>. It quickly becomes
clear that this is naturally a recursive lighting calculation, and if we ignore the energy dissipation of photons, it may even be an infinitely recursive lighting calculation.
Of course this would be intractably difficult to mirror, thus we will first initialize the constraint that we consider only <code>max_ray_depth</code> bounce backs of indirect
lighting, before we cut the ray and stop the recursion.
</p>
</div>

<div align = "justify">
<p class = "medium">
Now for ray <code>r</code>, once it reaches <code>p</code>, we first need to see whether this ray has recursed enough times. For each ray, we maintain a field <code>depth</code>
which in practice tracks the number of back bounces we’ve associated with this ray. If the depth is now 1, this means we’ve bounced back so many times, we no longer will recurse,
thus we are safe to return simply the direct lighting at <code>p</code>; we can utilize the direct lighting estimations from part 3 to determine the direct lighting radiance
rooted at <code>p</code>. Theoretically, we are free to use UHL or LIS, but in practice, there is little reason to use UHL. If the depth isn’t 1, this means we still have 2nd
degree lighting to compute, so we will sample the bsdf of <code>p</code>, to determine the direction of the next ray to trace, with the condition that the “out direction” is the
opposite of <code>r</code>’s direction. Of course <code>p</code> may be directly lit, so even if we wish to recurse further, we’ll still calculate the direct lighting at <code>p</code>.
</p>
</div>

<div align = "justify">
<p class = "medium">
While sampling this direction, we will also capture the pdf, the probability density function evaluated at the direction of the next ray to trace. Now we will also check to see
if the newly cast ray intersects with an object <code>q</code> (the new ray’s depth is one less than r’s depth). If it doesn’t intersect any object <code>q</code>, logically no
light can bounce from a nonexistent object to <code>p</code>, so no need to calculate any indirect lighting from this bounce back. If we do however intersect some object
<code>q</code>, we know by means of the reflection equation that the radiance rooted at <code>q</code> that transports to <code>p</code> is precisely:
<code>bsdf(dir_camera, dir_sampled) * cos(angle from sampled_dir to p) * at_least_one_bounce_radiance(ray_to_q, intersection_with_q) / pdf</code>. Thus the return value of the
function <code>at_least_one_bounce_radiance</code> is precisely the radiance that will be transported from <code>p</code> that requires at least a singular bounce
(without <code>p</code>’s light emission), or in other words, the sum of the previous expression, and the direct illumination of <code>p</code>.
</p>
</div>

<div align = "justify">
<p class = "medium">
This means that the global illumination that will travel counter to ray <code>r</code> that will hit the camera if <code>r</code> intersects primitive <code>p</code>, is
precisely the <code>zero_bounce_radiance(p, isect) + at_least_one_bounce_radiance(p, isect)</code> if <code>r</code> intersects <code>p</code> with intersection <code>isect</code>.
</p>
</div>

<br><br>
<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task4/CBspheres_DL.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae Direct Lighting Only</figcaption>
      </td>
      <td>
        <img src="images/task4/CBspheres_ID.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae Indirect Lighting Only</figcaption>
      </td>
    </tr>
  </table>
</div>
<br><br>

<div align = "justify">
<p class = "medium">
There is one more small note. Realistically, we don’t have to reach the <code>max_ray_depth</code> each time. Statistically speaking, we hit the point of diminishing return well
before this, so we implement a Russian Roulette algorithm. As the name suggests, before we recurse, with some probability (0.40 in my implementation), we cut the recursion, and
just return the radiance computed without another bounce back. The only difference this makes is that we also need to divide the q-rooted radiance equation by the probability of
back bounce, in this case 0.60. We save on some computation time, and statistically, we’ll be looking at basically the same render.
</p>
</div>

<div align = "left">
<p class = "medium">
<b><i>Maximal Ray Depth Comparison (1024 Samples Per Pixel):</i></b>
</div>
</p>

<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task4/CBbunny_1024_m0.png" align="middle" width="500"/>
        <figcaption align="middle">CBbunny.dae Ray Depth: 0</figcaption>
      </td>
      <td>
        <img src="images/task4/CBbunny_1024_m1.png" align="middle" width="500"/>
        <figcaption align="middle">CBbunny.dae Ray Depth: 1</figcaption>
      </td>
    </tr>
  </table>
</div>

<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task4/CBbunny_1024_m2.png" align="middle" width="500"/>
        <figcaption align="middle">CBbunny.dae Ray Depth: 2</figcaption>
      </td>
      <td>
        <img src="images/task4/CBbunny_1024_m3.png" align="middle" width="500"/>
        <figcaption align="middle">CBbunny.dae Ray Depth: 3</figcaption>
      </td>
    </tr>
  </table>
</div>

<div align = "middle">
<img src="images/task4/CBbunny_1024_m100.png" align="middle" width="500"/>
<figcaption align="middle">CBbunny.dae Ray Depth: 100</figcaption>
</div>

<br><br>


<div align = "left">
<p class = "medium">
<b><i>Pixel Sample Rate Comparison (4 Light Rays):</i></b>
</div>
</p>

<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task4/spheres_s1.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae: 1 Sample per Pixel</figcaption>
      </td>
      <td>
        <img src="images/task4/spheres_s2.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae: 2 Samples per Pixel</figcaption>
      </td>
    </tr>
  </table>
</div>

<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task4/spheres_s4.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae: 4 Samples per Pixel</figcaption>
      </td>
      <td>
        <img src="images/task4/spheres_s8.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae: 8 Samples per Pixel</figcaption>
      </td>
    </tr>
  </table>
</div>

<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task4/spheres_s16.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae: 16 Samples per Pixel</figcaption>
      </td>
      <td>
        <img src="images/task4/spheres_s64.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae: 64 Samples per Pixel</figcaption>
      </td>
    </tr>
  </table>
</div>

<div align = "middle">
<img src="images/task4/Spheres_HQ.png" align="middle" width="500"/>
<figcaption align="middle">CBspheres.dae: 1024 Samples per Pixel</figcaption>
</div>

<br><br>


<h2 align="middle">Section III: Statistical Improvement</h2>

<h3 align="middle">Part 5: Adaptive Sampling:</h3>

<br><br>

<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task5/CBspheres_lambertian.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae: 2048 Samples per Pixel (Adaptive Sampling)</figcaption>
      </td>
      <td>
        <img src="images/task5/CBspheres_lambertian_rate.png" align="middle" width="500"/>
        <figcaption align="middle">CBspheres.dae: Adaptive Sampling Rate</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<div align = "middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task5/CBbunny.png" align="middle" width="500"/>
        <figcaption align="middle">CBbunny.dae: 2048 Samples per Pixel (Adaptive Sampling)</figcaption>
      </td>
      <td>
        <img src="images/task5/CBbunny_rate.png" align="middle" width="500"/>
        <figcaption align="middle">CBbunny.dae: Adaptive Sampling Rate</figcaption>
      </td>
    </tr>
  </table>
</div>

<br><br>

<div align = "justify">
<p class = "medium">
We already got a glimpse into how we can leverage statistics to improve renders with part 4’s Russian Roulette algorithm, and in that case we got a faster render time, but this
doesn’t always mean statistical analysis and iteration of algorithms are used for speed. In fact, adaptive sampling trades longer render times (sometimes extremely longer) for
clearer quality renders that cancel some of the noise.
</p>
</div>

<div align = "justify">
<p class = "medium">
Intuitively, adaptive sampling is testing whether or not our illumination has been constrained. We’ll generate a confidence interval, where a compressed confidence interval
suggests we are reasonably sure that the illuminance will not be altered much by additional sampling, and a wide confidence interval suggests the opposite. This way, we can
<b>adapt</b> the sampling frequency to prevent it from being unnecessarily static.
</p>
</div>

<div align = "justify">
<p class = "medium">
My implementation of adaptive sampling is a direct encoding of the adaptive sampling algorithm. We will terminate tracing rays through a pixel if
<code>I <= maxTolerance * avg_illuminance; I = 1.96 * SD_illuminance / sqrt(sample_count)</code>.  When tracing the <code>num_samples</code> rays through pixel <code>(x, y)</code>,
we keep tracker variables <code>s1</code> and <code>s2</code>, where <code>s1</code> is the sum of the illuminance samples of pixel <code>(x, y)</code>, and <code>s2</code> is
the sum of the squares of the illuminance samples of pixel <code>(x, y)</code>).
</p>
</div>

<div align = "justify">
<p class = "medium">
We’ll batch sample to optimize our efficiency. What this means is that we will sample <code>samplesPerBatch</code> pixels each time, then test our termination statement.
From <code>s1</code> and <code>s2</code>, we can calculate the average illumination from <code>s1 / sample_count</code>, and the variance (which is just the square of the SD)
from <code>(1 / (sample_count - 1)) * (s2 - s1 * s1 / n)</code>. If our termination statement passes, we’ll simply end our Monte Carlo estimation earlier, otherwise, we’ll
keep sampling before we max out at <code>ns_aa</code> samples.
</p>
</div>

<br><br>
